# ğŸš€ Training Quick Start

## âœ… **All Scripts Ready!**

### **Available Training Scripts**

```
scripts/
â”œâ”€â”€ lstm/
â”‚   â”œâ”€â”€ test_lstm.sh              âš¡ Quick test (15 min)
â”‚   â”œâ”€â”€ train_lstm_baseline.sh    ğŸ¯ Baseline (1 hour)
â”‚   â””â”€â”€ train_lstm_augmented.sh   ğŸ“Š Extended (12 hours)
â”œâ”€â”€ distilbert/
â”‚   â”œâ”€â”€ test_distilbert.sh        âš¡ Quick test (20 min)
â”‚   â””â”€â”€ train_distilbert.sh       ğŸ¤– Transformer (3 hours)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ augment_dataset.py
â”‚   â”œâ”€â”€ organize_data.py
â”‚   â””â”€â”€ data_quickref.sh
â””â”€â”€ run_training.sh               ğŸ® Interactive launcher
```

---

## ğŸ® **Easy Way: Use the Launcher**

```bash
# Interactive menu
bash scripts/run_training.sh

# Or directly specify
bash scripts/run_training.sh 1              # Test LSTM
bash scripts/run_training.sh test-lstm      # Test LSTM
bash scripts/run_training.sh distilbert     # Train DistilBERT
```

---

## âš¡ **Quick Start: Test First!**

### **1. Test LSTM** (recommended first step)
```bash
sbatch scripts/lstm/test_lstm.sh

# Check status
squeue -u $USER

# Monitor logs
tail -f logs/test-lstm_*.out
```

**What it does:**
- Uses 17 files from `data/test`
- Runs 3 iterations
- Takes ~15 minutes
- Verifies your setup works

### **2. Test DistilBERT** (if LSTM passed)
```bash
sbatch scripts/distilbert/test_distilbert.sh

# Monitor
tail -f logs/test-distilbert_*.out
```

**What it does:**
- Uses 17 files from `data/test`
- Runs 3 iterations
- Takes ~20 minutes
- Verifies transformer works

---

## ğŸ¯ **Full Training: Choose Your Model**

### **Option A: LSTM Baseline** (fastest)
```bash
sbatch scripts/lstm/train_lstm_baseline.sh
```
- â±ï¸ **Time**: ~1 hour
- ğŸ“ **Data**: 9,441 files
- ğŸ¯ **Use**: Baseline comparison

### **Option B: LSTM Augmented** (best results)
```bash
sbatch scripts/lstm/train_lstm_augmented.sh
```
- â±ï¸ **Time**: ~12 hours
- ğŸ“ **Data**: 9,941 files (includes augmentation)
- ğŸ¯ **Use**: Best LSTM performance

### **Option C: DistilBERT** (transformer)
```bash
sbatch scripts/distilbert/train_distilbert.sh
```
- â±ï¸ **Time**: ~3 hours (slower but powerful)
- ğŸ“ **Data**: 9,441 files
- ğŸ¯ **Use**: Transformer approach

---

## ğŸ“Š **Data Available**

| Dataset | Location | Files | Description |
|---------|----------|-------|-------------|
| Test | `data/test` | 17 | Quick validation |
| Training | `data/all` | 9,441 | Main dataset |
| Augmented | `data/generated` | 500 | Extra diversity |
| **Total** | - | **9,958** | All available |

---

## ğŸ“‹ **Corresponding Configs**

Each script uses a specific config file:

| Script | Config File |
|--------|-------------|
| `lstm/test_lstm.sh` | `config/test.json` |
| `distilbert/test_distilbert.sh` | `config/test_distilbert.json` |
| `lstm/train_lstm_baseline.sh` | `config/config.json` |
| `lstm/train_lstm_augmented.sh` | `config/config_augmented.json` |
| `distilbert/train_distilbert.sh` | `config/config_distilbert.json` |

---

## ğŸ“ˆ **Monitor Your Training**

### **Check Job Status**
```bash
squeue -u $USER
```

### **View Logs**
```bash
# Live monitoring
tail -f logs/train-lstm-baseline_*.out

# View all logs
ls -lh logs/

# Check errors
cat logs/train-lstm-baseline_*.err
```

### **Check Results**
```bash
# List LSTM results
ls -lh results/lstm/

# List DistilBERT results
ls -lh results/distilbert/

# View saved models
ls -lh results/lstm/run_*/models/
ls -lh results/distilbert/run_*/models/
```

---

## ğŸ¯ **Recommended Order**

1. âœ… **Test LSTM** â†’ Verify setup (15 min)
2. âœ… **Test DistilBERT** â†’ Verify transformer (20 min)
3. ğŸ¯ **Train baseline** â†’ Get baseline metrics (1 hour)
4. ğŸ“ˆ **Evaluate models** â†’ Measure performance (10-20 min)
5. ğŸ“Š **Compare models** â†’ Analyze results
6. ğŸš€ **Full training** â†’ Best model + augmentation

---

## ğŸ“ˆ **Evaluation**

After training, evaluate your models:

### **Evaluate LSTM**
```bash
# Evaluate latest run
sbatch scripts/lstm/eval_lstm.sh

# Evaluate specific run
export EVAL_DIR=results/lstm/run_0
sbatch scripts/lstm/eval_lstm.sh
```

### **Evaluate DistilBERT**
```bash
# Evaluate latest run
sbatch scripts/distilbert/eval_distilbert.sh

# Evaluate specific run
export EVAL_DIR=results/distilbert/run_0
sbatch scripts/distilbert/eval_distilbert.sh
```

### **View Evaluation Results**
```bash
# Check evaluation logs
cat results/lstm/run_0/logs/eval/average_speedup
cat results/distilbert/run_0/logs/eval/average_speedup
```

---

## ğŸ› ï¸ **Troubleshooting**

### **Script won't run**
```bash
chmod +x scripts/*.sh
```

### **Job pending forever**
```bash
scontrol show job [JOB_ID]
```

### **Out of memory**
- Start with test scripts
- Reduce batch size in config
- Request more memory in script

### **Training fails**
```bash
# Check error logs
cat logs/[job-name]_*.err

# Validate config
python tests/test_config_loading.py
```

---

## ğŸ“š **Documentation**

- **Scripts Guide**: `scripts/TRAINING_GUIDE.md`
- **Results Guide**: `results/README.md`
- **Config Guide**: `config/README.md`
- **Project Roadmap**: `docs/ROADMAP.md`
- **Config Update**: `docs/CONFIG_UPDATE_SUMMARY.md`

---

## ğŸ‰ **You're Ready!**

Start with:
```bash
bash scripts/run_training.sh
```

Then choose option **1** to test LSTM!
